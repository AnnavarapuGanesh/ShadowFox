# 🔥 ShadowFox AI/ML Internship - Weekly Tasks by Annavarapu Ganesh

Welcome to my ShadowFox Internship GitHub repository. This repository contains all the completed tasks for Weeks 1 to 3 of the AI/ML Internship Program, including implementation, explanations, and outputs.

---

## 👨‍💻 Author

**Name**: Annavarapu Ganesh  
**Email**: annavarapuganesh45@gmail.com  
**GitHub**: [github.com/AnnavarapuGanesh](https://github.com/AnnavarapuGanesh)  
**LinkedIn**: [linkedin.com/in/annavarapu-ganesh-4159732a5](https://www.linkedin.com/in/annavarapu-ganesh-4159732a5/)

---

## 📅 Week 1: Boston House Price Prediction 🏠

### ✅ Objective
Build a regression model to predict house prices based on features like number of rooms, location, and area using the Boston housing dataset.

### 🔧 Tech Stack
- Python
- Pandas, NumPy, Seaborn, Matplotlib
- Scikit-learn (Linear Regression, Decision Tree, Gradient Boosting)

### 📂 Contents
- Data Cleaning
- Feature Scaling
- Outlier Handling
- Model Training & Evaluation
- Cross-Validation & Hyperparameter Tuning

### 📈 Outcome
Achieved optimized regression performance using ensemble methods and explained model evaluation using R² Score and MAE.

---

## 📅 Week 2: Loan Approval Prediction 🏦

### ✅ Objective
Classify whether a loan will be approved based on customer features like income, credit history, employment status.

### 🔧 Tech Stack
- Python
- Pandas, NumPy, Seaborn
- Scikit-learn (Logistic Regression, Random Forest, XGBoost)
- Label Encoding, OneHot Encoding, SMOTE

### 📂 Contents
- Exploratory Data Analysis (EDA)
- Imbalanced Data Handling
- Model Training and ROC-AUC Evaluation
- Confusion Matrix & Classification Reports

### 📈 Outcome
Built a robust classification model with over 85% accuracy and ROC-AUC improvement using XGBoost.

---

## 📅 Week 3: GPT-2 Text Generation with NLP 🤖

### ✅ Objective
Deploy and analyze a transformer-based language model (GPT-2) for various text generation tasks.

### 🔧 Tech Stack
- Python
- HuggingFace Transformers (GPT-2)
- PyTorch
- Matplotlib, WordCloud

### 📂 Features
- Implemented GPT-2 using HuggingFace
- Generated outputs from prompts across different domains
- Evaluated temperature effects and output length
- Visualized word count and model creativity

### 📊 Research Highlights
- Explored how temperature affects creativity vs coherence
- Compared GPT-2’s performance in technical, educational, and creative texts
- Benchmarked model speed and memory usage

### 📈 Results
GPT-2 generated high-quality, coherent responses with customizable creativity using temperature tuning.

---

## 🌐 Reference
- [roadmap.sh/ai-data-scientist](https://roadmap.sh/ai-data-scientist)

---


